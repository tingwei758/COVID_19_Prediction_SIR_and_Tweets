{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from nltk.corpus import stopwords\n",
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf = SparkConf().set(\"spark.jars\", \"./spark-nlp_2.11-2.4.5.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.2.13:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark NLP</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1a211ea190>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.5'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparknlp.version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read\\\n",
    "    .option(\"encoding\", \"UTF-8\")\\\n",
    "    .option(\"delimiter\", \",\")\\\n",
    "    .option(\"parserLib\", \"univocity\")\\\n",
    "    .option(\"multiLine\", \"true\")\\\n",
    "    .option(\"escape\", \"\\\"\")\\\n",
    "    .csv(\"tweets\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1238253442063310848</td>\n",
       "      <td>532343475</td>\n",
       "      <td>2020-03-13T00:00:00Z</td>\n",
       "      <td>The UFC is about to be the most popular sport ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1238253441778098177</td>\n",
       "      <td>165879150</td>\n",
       "      <td>2020-03-13T00:00:00Z</td>\n",
       "      <td>The great toilet paper depression of 2020 #Toi...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1238253440486313988</td>\n",
       "      <td>569242704</td>\n",
       "      <td>2020-03-13T00:00:00Z</td>\n",
       "      <td>The 'Spotlight Show' with @janeyleegrace on @u...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1238253439051870208</td>\n",
       "      <td>16368021</td>\n",
       "      <td>2020-03-13T00:00:00Z</td>\n",
       "      <td>Because we all the time in the world right? @s...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1238253440821649408</td>\n",
       "      <td>1057148786189824000</td>\n",
       "      <td>2020-03-13T00:00:00Z</td>\n",
       "      <td>French pastry chef shows off Easter eggs model...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1238253442034020354</td>\n",
       "      <td>1093544067219292161</td>\n",
       "      <td>2020-03-13T00:00:00Z</td>\n",
       "      <td>ICYMI - Hour 2 of #TheGamePlan with @DaveWNSP ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1238253441564266496</td>\n",
       "      <td>39743812</td>\n",
       "      <td>2020-03-13T00:00:00Z</td>\n",
       "      <td>With rising #Coronavirus cases in India, which...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1238253441517928448</td>\n",
       "      <td>17852186</td>\n",
       "      <td>2020-03-13T00:00:00Z</td>\n",
       "      <td>#ICYMI: #Ontario #MPPs may temporarily suspend...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1238253440603541504</td>\n",
       "      <td>1454687180</td>\n",
       "      <td>2020-03-13T00:00:00Z</td>\n",
       "      <td>Despite having only 3 confirmed #coronavirus c...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1238253440461135873</td>\n",
       "      <td>19047089</td>\n",
       "      <td>2020-03-13T00:00:00Z</td>\n",
       "      <td>Autonomous #Robots Are Helping Kill #Coronavir...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             status_id              user_id            created_at  \\\n",
       "0  1238253442063310848            532343475  2020-03-13T00:00:00Z   \n",
       "1  1238253441778098177            165879150  2020-03-13T00:00:00Z   \n",
       "2  1238253440486313988            569242704  2020-03-13T00:00:00Z   \n",
       "3  1238253439051870208             16368021  2020-03-13T00:00:00Z   \n",
       "4  1238253440821649408  1057148786189824000  2020-03-13T00:00:00Z   \n",
       "5  1238253442034020354  1093544067219292161  2020-03-13T00:00:00Z   \n",
       "6  1238253441564266496             39743812  2020-03-13T00:00:00Z   \n",
       "7  1238253441517928448             17852186  2020-03-13T00:00:00Z   \n",
       "8  1238253440603541504           1454687180  2020-03-13T00:00:00Z   \n",
       "9  1238253440461135873             19047089  2020-03-13T00:00:00Z   \n",
       "\n",
       "                                                text lang  \n",
       "0  The UFC is about to be the most popular sport ...   en  \n",
       "1  The great toilet paper depression of 2020 #Toi...   en  \n",
       "2  The 'Spotlight Show' with @janeyleegrace on @u...   en  \n",
       "3  Because we all the time in the world right? @s...   en  \n",
       "4  French pastry chef shows off Easter eggs model...   en  \n",
       "5  ICYMI - Hour 2 of #TheGamePlan with @DaveWNSP ...   en  \n",
       "6  With rising #Coronavirus cases in India, which...   en  \n",
       "7  #ICYMI: #Ontario #MPPs may temporarily suspend...   en  \n",
       "8  Despite having only 3 confirmed #coronavirus c...   en  \n",
       "9  Autonomous #Robots Are Helping Kill #Coronavir...   en  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\")\n",
    "\n",
    "# sentence_detector = SentenceDetector() \\\n",
    "#     .setInputCols([\"document\"]) \\\n",
    "#     .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"normal\")\\\n",
    "    .setCleanupPatterns(['[^A-Za-z]', 'http.*'])\\\n",
    "    .setLowercase(True)\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "    .setInputCols([\"normal\"]) \\\n",
    "    .setOutputCol(\"clean\") \\\n",
    "    .setCaseSensitive(False) \\\n",
    "    .setStopWords(eng_stopwords)\n",
    "\n",
    "# lemmatizer = LemmatizerModel.pretrained() \\\n",
    "#     .setInputCols([\"clean\"]) \\\n",
    "#     .setOutputCol(\"lemma\")\n",
    "\n",
    "lemmatizer=LemmatizerModel.load('lemma_antbnc_en_2.0.2_2.4_1556480454569/')\\\n",
    "    .setInputCols([\"clean\"]) \\\n",
    "    .setOutputCol(\"lemma\")\n",
    "\n",
    "stemmer = Stemmer() \\\n",
    "    .setInputCols([\"lemma\"]) \\\n",
    "    .setOutputCol(\"stem\")\n",
    "\n",
    "# word_embeddings=ElmoEmbeddings.pretrained()\\\n",
    "#     .setInputCols([\"document\", \"lemma\"])\\\n",
    "#     .setOutputCol(\"embedding\")\n",
    "\n",
    "# word_embeddings=ElmoEmbeddings.load('elmo_en_2.4.0_2.4_1580488815299/')\\\n",
    "#     .setInputCols([\"document\", \"lemma\"])\\\n",
    "#     .setOutputCol(\"embedding\")\n",
    "\n",
    "\n",
    "# sentence_embeddings=SentenceEmbeddings()\\\n",
    "#     .setInputCols([\"document\", \"embedding\"])\\\n",
    "#     .setOutputCol(\"sentence_embedding\")\\\n",
    "#     .setPoolingStrategy(\"AVERAGE\")\n",
    "\n",
    "\n",
    "ngrams_cum = NGramGenerator() \\\n",
    "    .setInputCols([\"stem\"]) \\\n",
    "    .setOutputCol(\"ngram\") \\\n",
    "    .setN(2) \\\n",
    "    .setEnableCumulative(True)\\\n",
    "    .setDelimiter(\"_\") # Default is space\n",
    "\n",
    "\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"ngram\"]) \\\n",
    "    .setOutputCols(['tokens'])\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler, \n",
    "#     sentence_detector, \n",
    "    tokenizer, \n",
    "    normalizer, \n",
    "    stopwords_cleaner,\n",
    "    lemmatizer,\n",
    "    stemmer,\n",
    "#     word_embeddings,\n",
    "#     sentence_embeddings,\n",
    "    ngrams_cum,\n",
    "    finisher\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.select(\n",
    "        'status_id',\n",
    "        'created_at',\n",
    "        F.from_unixtime(F.unix_timestamp('created_at', 'yyyy-MM-dd')).cast('timestamp').alias('date'),\n",
    "        F.explode(\"tokens\").alias(\"word\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.groupBy(['word', 'date'])\\\n",
    "    .agg(F.countDistinct(\"status_id\").alias(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>date</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ufc</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>popular</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>sport</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>entir</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>world</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6636</td>\n",
       "      <td>univers_amp</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6637</td>\n",
       "      <td>self</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6638</td>\n",
       "      <td>perhap</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6639</td>\n",
       "      <td>face_disciplin</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6640</td>\n",
       "      <td>bulk_order</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6641 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                word       date  count\n",
       "0                ufc 2020-03-13      1\n",
       "1            popular 2020-03-13      1\n",
       "2              sport 2020-03-13     15\n",
       "3              entir 2020-03-13      1\n",
       "4              world 2020-03-13     17\n",
       "...              ...        ...    ...\n",
       "6636     univers_amp 2020-03-13      1\n",
       "6637            self 2020-03-13      2\n",
       "6638          perhap 2020-03-13      1\n",
       "6639  face_disciplin 2020-03-13      1\n",
       "6640      bulk_order 2020-03-13      1\n",
       "\n",
       "[6641 rows x 3 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = result.toPandas()\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.write\\\n",
    "    .option(\"encoding\", \"UTF-8\")\\\n",
    "    .option(\"delimiter\", \",\")\\\n",
    "    .option(\"parserLib\", \"univocity\")\\\n",
    "    .option(\"multiLine\", \"true\")\\\n",
    "    .option(\"escape\", \"\\\"\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .parquet(\"word_count.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.read\\\n",
    "    .option(\"encoding\", \"UTF-8\")\\\n",
    "    .option(\"delimiter\", \",\")\\\n",
    "    .option(\"parserLib\", \"univocity\")\\\n",
    "    .option(\"multiLine\", \"true\")\\\n",
    "    .option(\"escape\", \"\\\"\")\\\n",
    "    .parquet(\"word_count.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = result.groupby('word')\\\n",
    "    .agg(F.sum('count').alias('count'))\\\n",
    "    .orderBy(F.desc('count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [row[0] for row in result2.limit(1000).select('word').collect()]\n",
    "if 'date' in words:\n",
    "    words.remove('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "result3 = result.groupBy('date').pivot('word', words).max('count').orderBy('date').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>coronaviru</th>\n",
       "      <th>covid</th>\n",
       "      <th>coronaviruspandem</th>\n",
       "      <th>peopl</th>\n",
       "      <th>get</th>\n",
       "      <th>go</th>\n",
       "      <th>ne</th>\n",
       "      <th>time</th>\n",
       "      <th>spread</th>\n",
       "      <th>...</th>\n",
       "      <th>french</th>\n",
       "      <th>egg</th>\n",
       "      <th>pastri_chef</th>\n",
       "      <th>easteregg_coronaviru</th>\n",
       "      <th>ncaatourna</th>\n",
       "      <th>hour_thegameplan</th>\n",
       "      <th>nfl_ncaa</th>\n",
       "      <th>railwai</th>\n",
       "      <th>rise_coronaviru</th>\n",
       "      <th>effort_bid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>239</td>\n",
       "      <td>143</td>\n",
       "      <td>77</td>\n",
       "      <td>32</td>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 1001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  coronaviru  covid  coronaviruspandem  peopl  get  go  ne  time  \\\n",
       "0 2020-03-13         239    143                 77     32   29  27  24    24   \n",
       "\n",
       "   spread  ...  french  egg  pastri_chef  easteregg_coronaviru  ncaatourna  \\\n",
       "0      23  ...       1    1            1                     1           1   \n",
       "\n",
       "   hour_thegameplan  nfl_ncaa  railwai  rise_coronaviru  effort_bid  \n",
       "0                 1         1        1                1           1  \n",
       "\n",
       "[1 rows x 1001 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result3.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases = spark.read.csv('covid19.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases = df_cases.filter(F.col('prname') == 'Canada')\\\n",
    "    .select(\n",
    "        F.from_unixtime(F.unix_timestamp('date', 'dd-MM-yyyy')).cast('timestamp').alias('date'),\n",
    "        F.col('numconf').cast('Long'),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "result4 = result3.join(df_cases, on='date', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result4.stat.corr(\"coronaviru\", \"numconf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(coronaviru=239)]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result4.select(\"coronaviru\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/tingwei758/opt/anaconda3/envs/pyspark/lib/python3.7/site-packages/pyspark/serializers.py\", line 597, in dumps\n",
      "    return cloudpickle.dumps(obj, 2)\n",
      "  File \"/Users/tingwei758/opt/anaconda3/envs/pyspark/lib/python3.7/site-packages/pyspark/cloudpickle.py\", line 863, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/Users/tingwei758/opt/anaconda3/envs/pyspark/lib/python3.7/site-packages/pyspark/cloudpickle.py\", line 260, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"/Users/tingwei758/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\", line 437, in dump\n",
      "    self.save(obj)\n",
      "  File \"/Users/tingwei758/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/Users/tingwei758/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\", line 789, in save_tuple\n",
      "    save(element)\n",
      "  File \"/Users/tingwei758/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/Users/tingwei758/opt/anaconda3/envs/pyspark/lib/python3.7/site-packages/pyspark/cloudpickle.py\", line 406, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/Users/tingwei758/opt/anaconda3/envs/pyspark/lib/python3.7/site-packages/pyspark/cloudpickle.py\", line 549, in save_function_tuple\n",
      "    save(state)\n",
      "  File \"/Users/tingwei758/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/Users/tingwei758/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\", line 859, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/Users/tingwei758/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\", line 885, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/Users/tingwei758/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/Users/tingwei758/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\", line 819, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/Users/tingwei758/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\", line 846, in _batch_appends\n",
      "    save(tmp[0])\n",
      "  File \"/Users/tingwei758/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/Users/tingwei758/opt/anaconda3/envs/pyspark/lib/python3.7/site-packages/pyspark/cloudpickle.py\", line 400, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/Users/tingwei758/opt/anaconda3/envs/pyspark/lib/python3.7/site-packages/pyspark/cloudpickle.py\", line 549, in save_function_tuple\n",
      "    save(state)\n",
      "  File \"/Users/tingwei758/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/Users/tingwei758/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\", line 859, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/Users/tingwei758/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\", line 885, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/Users/tingwei758/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/Users/tingwei758/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\", line 859, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/Users/tingwei758/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\", line 890, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/Users/tingwei758/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\", line 549, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/Users/tingwei758/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\", line 662, in save_reduce\n",
      "    save(state)\n",
      "  File \"/Users/tingwei758/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/Users/tingwei758/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\", line 859, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/Users/tingwei758/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\", line 885, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/Users/tingwei758/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\", line 524, in save\n",
      "    rv = reduce(self.proto)\n",
      "  File \"/Users/tingwei758/opt/anaconda3/envs/pyspark/lib/python3.7/site-packages/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/Users/tingwei758/opt/anaconda3/envs/pyspark/lib/python3.7/site-packages/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/tingwei758/opt/anaconda3/envs/pyspark/lib/python3.7/site-packages/py4j/protocol.py\", line 332, in get_return_value\n",
      "    format(target_id, \".\", name, value))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o49060.__getstate__. Trace:\n",
      "py4j.Py4JException: Method __getstate__([]) does not exist\n",
      "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n",
      "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:274)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: Py4JError: An error occurred while calling o49060.__getstate__. Trace:\npy4j.Py4JException: Method __getstate__([]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n\tat py4j.Gateway.invoke(Gateway.java:274)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/site-packages/pyspark/serializers.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcloudpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPickleError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/site-packages/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, protocol)\u001b[0m\n\u001b[1;32m    862\u001b[0m         \u001b[0mcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCloudPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m         \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/site-packages/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_framing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTOP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave_tuple\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    788\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/site-packages/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36msave_function\u001b[0;34m(self, obj, name)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mklass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mklass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_function_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/site-packages/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36msave_function_tuple\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'qualname'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTUPLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    884\u001b[0m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave_list\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_appends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36m_batch_appends\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    845\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m                 \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAPPEND\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/site-packages/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36msave_function\u001b[0;34m(self, obj, name)\u001b[0m\n\u001b[1;32m    399\u001b[0m                 or themodule is None):\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_function_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/site-packages/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36msave_function_tuple\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'qualname'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTUPLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    884\u001b[0m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m                 \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    891\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# Save the reduce() output and finally memoize the object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, obj)\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m             \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m             \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBUILD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    884\u001b[0m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m                 \u001b[0mrv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    331\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}. Trace:\\n{3}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m                     format(target_id, \".\", name, value))\n\u001b[0m\u001b[1;32m    333\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o49060.__getstate__. Trace:\npy4j.Py4JException: Method __getstate__([]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n\tat py4j.Gateway.invoke(Gateway.java:274)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-150-83b839f3cd58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresult4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36m_jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2531\u001b[0m         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\n\u001b[0;32m-> 2532\u001b[0;31m                                       self._jrdd_deserializer, profiler)\n\u001b[0m\u001b[1;32m   2533\u001b[0m         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,\n\u001b[1;32m   2534\u001b[0m                                              self.preservesPartitioning, self.is_barrier)\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   2432\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"serializer should not be empty\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2433\u001b[0m     \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofiler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2434\u001b[0;31m     \u001b[0mpickled_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcast_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincludes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_for_python_RDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2435\u001b[0m     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n\u001b[1;32m   2436\u001b[0m                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;31m# the serialized command will be compressed by broadcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m     \u001b[0mser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCloudPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0mpickled_command\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickled_command\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m<<\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 1M\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m         \u001b[0;31m# The broadcast will have same life cycle as created PythonRDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/site-packages/pyspark/serializers.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    605\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Could not serialize object: %s: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0mcloudpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_exec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: Py4JError: An error occurred while calling o49060.__getstate__. Trace:\npy4j.Py4JException: Method __getstate__([]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n\tat py4j.Gateway.invoke(Gateway.java:274)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n"
     ]
    }
   ],
   "source": [
    "sc.parallelize(words).map(lambda w: result4.select(w).toPandas()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
